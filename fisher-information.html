<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8" />
  <title>fisher-information</title>
  <!-- Load MathJax (version 3) from a public CDN -->
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>
</head>
<body>

  <h2>Fisher Information</h2>

  <!-- Main text block (content unchanged) -->
  <p>
    The \textbf{Fisher Information} or the information \(\mathcal{I}(\theta)\) is the 
    variance of the score 
    \(\mathrm{Var}\!\bigl(\tfrac{\partial \log L(\theta \mid x)}{\partial \theta}\bigr)\) 
    at the true unknown parameter \(\theta\). It can also be shown to be equal to 
    \(-E\!\bigl[\tfrac{\partial^2 \log L(\theta \mid x)}{\partial \theta \,\partial \theta^T}\bigr]\),
    the negative average of the second derivative of the log likelihood function, 
    which is a useful result. The Fisher information derives its name from how it 
    describes how well estimated the true unknown parameter value is given the data. 
    If the Fisher information has a high value then there is high variance around the 
    true unknown parameter, making its value easy to identify. On the other hand, if 
    the matrix returns a low value then the true unknown parameter is harder to 
    identify. We thus state that Fisher's metric provides a sense of 'information' 
    about the true unknown parameter(s) from the data, and when we have multiple 
    parameters we call this quantity the \textbf{Fisher Information Matrix} (or 
    Fisher matrix, Information matrix etc). Further, in the multivariate case, we 
    are interested in the diagonal elements as they will tell us the variance of the 
    score about each of the true unknown parameters, whereas the off diagonal entries 
    will tell us the covariance. We are interested in the multivariate case, the 
    Fisher matrix, since it may be applied to general results. To this end, we derive 
    its expression as the variance of the score about the true unknown parameters.
  </p>

  <p>
    Recall that variance is given by 
    \(E\bigl[(X - E[X])^2\bigr] = E[X^2] - \bigl(E[X]\bigr)^2\). Substituting in the 
    score we have 
    \(E\bigl[s(\theta \mid x)^2\bigr] - \bigl(E[s(\theta \mid x)]\bigr)^2\). Let 
    \(\nabla_{\theta}\) denote the gradient with respect to \(\theta\), where 
    \(\theta\) is the true unknown parameter, and observe that
  </p>

  <!-- Displaying the centered equation. 
       We replace \begin{center}...\end{center} with a centered <p> containing \[...\] -->
  <p style="text-align:center;">
    \[
      (E[s(\theta \mid x)])^2 
      = \Bigl(\int \nabla_{\theta}\,\ln\bigl(f(x \mid \theta)\bigr)\,f(x \mid \theta)\,dx\Bigr)^2
    \]
  </p>

   <p>
    which by the properties of logarithm derivatives gives
  </p>

  <!-- Centered equation block 1 -->
  <p style="text-align:center;">
    \[
      (\int \frac{1}{f(x|\theta}\nabla_{\theta}f(x|\theta)f(x|\theta)dx)^2 
      = (\int \nabla f(x|\theta)dx)^2 
      = (\nabla_{\theta} \int f(x|\theta)dx)^2 
      = (\nabla_{\theta} 1)^2 
      = 0
    \]
  </p>

  <p>
    Then the Fisher matrix is given by 
    \(E[s(\theta|x)^2] - (E[s(\theta|x)])^2 = E[s(\theta|x)^2]\)
  </p>

  <p>
    Now, we want to demonstrate why the Fisher matrix may be written as the negative average of the second derivative
  </p>

  <!-- Centered equation block 2 -->
  <p style="text-align:center;">
    \[
      \nabla_{\theta}(s(\theta|x)) 
      = \nabla_{\theta}(\nabla_{\theta}(\log f(\theta|x))) 
      = \nabla_{\theta}\bigl(\frac{\nabla_{\theta} f(\theta|x)}{f(\theta|x)}\bigr)
    \]
  </p>

  <p>
    since we are taking the derivative of a logarithm. Because we now have the derivative of a quotient we may apply the quotient rule to obtain
  </p>

  <!-- Centered equation block 3 -->
  <p style="text-align:center;">
    \[
      \frac{f(\theta|x)\nabla_{\theta}^2(f(\theta|x)) - \nabla_{\theta}(f(\theta|x))\nabla_{\theta}(f(\theta|x))}{(f(\theta|x))^2}
    \]
  </p>

  <!-- Centered equation block 4 -->
  <p style="text-align:center;">
    \[
      = \frac{\nabla_{\theta}^2(f(\theta|x))}{(f(\theta|x))} 
        - \frac{(\nabla_{\theta}(f(\theta|x))\nabla_{\theta}(f(\theta|x))}{(f(\theta|x))^2}
    \]
  </p>

  <!-- Centered equation block 5 -->
  <p style="text-align:center;">
    \[
      = \frac{\nabla_{\theta}^2(f(\theta|x))}{(f(\theta|x))} 
        - (\nabla_{\theta}\log(f(\theta|x)))\nabla_{\theta}\log(f(\theta|x))
    \]
  </p>

  <p>
    by the derivative of logarithms. Also recall that the score \(s(\theta|x) = \frac{\partial \log L(\theta|x)}{\partial \theta}\) or \(\nabla_{\theta}\log L(\theta|x)\) in the multivariate case, which gives us
  </p>

  <!-- Centered equation block 6 -->
  <p style="text-align:center;">
    \[
      \frac{\nabla_{\theta}^2(f(\theta|x))}{(f(\theta|x))} 
      - (s(\theta|x))^2
    \]
  </p>

  <p>
    and
  </p>

  <!-- Centered equation block 7 -->
  <p style="text-align:center;">
    \[
      E[\frac{\nabla_{\theta}^2(L(\theta|x))}{(L(\theta|x))}]
    \]
  </p>

  <!-- Centered equation block 8 -->
  <p style="text-align:center;">
    \[
      = \int \frac{\nabla_{\theta}^2(f(\theta|x))}{f(\theta|x)} f(\theta|x)dx
    \]
  </p>

  <!-- Centered equation block 9 -->
  <p style="text-align:center;">
    \[
      = \int \nabla_{\theta}^2 f(\theta|x) dx 
      = \nabla_{\theta}^2 \int f(\theta|x) dx
    \]
  </p>

  <!-- Centered equation block 10 -->
  <p style="text-align:center;">
    \[
      = \nabla_{\theta}^2 \int f(x|\theta) dx 
      = \nabla_{\theta}^2 1 
      = 0
    \]
  </p>

  <p>
    then we have
  </p>

  <!-- Centered equation block 11 -->
  <p style="text-align:center;">
    \[
      E[\nabla_{\theta}^2 \log(f(\theta|x))] 
      = E[\frac{\nabla_{\theta}^2(f(\theta|x))}{(f(\theta|x))} - (s(\theta|x))^2]
    \]
  </p>

  <!-- Centered equation block 12 -->
  <p style="text-align:center;">
    \[
      = 0 - E[(s(\theta|x))^2]
    \]
  </p>

  <!-- Centered equation block 13 -->
  <p style="text-align:center;">
    \[
      = -\mathcal{I}(\theta)
    \]
  </p>

  <p>
    thus
  </p>

  <!-- Centered equation block 14 -->
  <p style="text-align:center;">
    \[
      \mathcal{I}(\theta) 
      = -E[\nabla_{\theta}^2 \log(L(\theta|x))]
    \]
  </p>

  <!-- Centered equation block 15 -->
  <p style="text-align:center;">
    \[
      = -H(x|\theta)
    \]
  </p>

  <p>
    where \(H(x|\theta)\) denotes the Hessian matrix, which is a square, symmetric 
    matrix every element of which is the second gradient of the log likelihood 
    function.
  </p>

  <p>
    <!-- Link back to home page (optional) -->
    <a href="index.html">Return to Home Page</a>
  </p>

</body>
</html>

<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <title>central-limit-theorem</title>
<!-- Load the MathJax library from a CDN -->
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>
</head>
<body>
  
 <h1>Lindeberg-Levy CLT</h1>
  
  <p>
    The Lindeberg-Levy CLT states that suppose we have some sample of random variables 
    \(X_1, X_2, X_3, \ldots\) drawn from a population, which are independent and 
    identically distributed (the value of one does not change the value of any 
    other and all have the same distribution), with sample mean \(\bar{X_i}\), 
    population mean \(E[X_i] = \mu\), and population variance 
    \(\mathrm{Var}(X_i) = \sigma^2 < \infty\) (finite variance). 
    Then as the sample size \(n\) approaches \(\infty\), the standardized sample means 
    \(\frac{\sqrt{n}(\bar{X} - \mu)}{\sigma}\) converge in distribution to the 
    standard normal distribution \(\mathcal{N}(0,1)\).
  </p>
  
  <p>
    In other words, the distribution of the sample mean, with samples drawn from some 
    population that has a mean and variance (first and second moments), will be 
    normally distributed for large enough sample size \(n\). To prove the CLT, we 
    consider the sample mean 
    \(\bar{X} = \frac{1}{n} \sum_{i=1}^{n} X_i\) 
    which, on average, is:
  </p>
  
  <!-- Display math (centered) -->
  <p style="text-align:center;">
    \[
      E[\bar{X}] 
      = E\!\Bigl[\tfrac{1}{n} \sum_{i=1}^{n} X_i\Bigr] 
      = \tfrac{1}{n} \sum_{i=1}^{n} E[X_i] 
      = \tfrac{1}{n} n\mu 
      = \mu
    \]
  </p>

  <p>
    and thus the average of the sample means is the population mean. 
    The variance of the sample mean is:
  </p>
  
  <!-- Display math (centered) -->
  <p style="text-align:center;">
    \[
      \mathrm{Var}(\bar{X}) 
      = \mathrm{Var}\!\Bigl(\tfrac{1}{n} \sum_{i=1}^{n} X_i\Bigr) 
      = \tfrac{1}{n^2} \sum_{i=1}^{n} \mathrm{Var}(X_i) 
      = \tfrac{1}{n^2} \sum_{i=1}^{n} \sigma^2 
      = \tfrac{\sigma^2}{n}
    \]
  </p>
  
  <p>
    Thus the mean of the sampling distribution of sample means 
    \(\bar{X}_i\) is equal to the population mean with variance 
    \(\frac{\sigma^2}{n}\). To standardize the sample mean we center it 
    around the mean of the sample means (the population mean) and divide 
    by its standard deviation, which gives:
  </p>
  
  <p style="text-align:center;">
    \[
      \frac{\bar{X} - \mu}{\frac{\sigma}{\sqrt{n}}} 
      = \frac{\sqrt{n}(\bar{X} - \mu)}{\sigma}
    \]
  </p>
  
  <p style="text-align:center;">
    \[
      = \frac{\sqrt{n}\Bigl(\tfrac{1}{n}\sum_{i=1}^{n} X_i - \mu\Bigr)}{\sigma}
    \]
  </p>
  
  <p style="text-align:center;">
    \[
      = \frac{\tfrac{1}{\sqrt{n}} \sum_{i=1}^{n} X_i - \sqrt{n}\,\mu}{\sigma}
    \]
  </p>
  
  <p>
    We can call the standardized sample mean by a new random variable 
    \(\bar{X}_s\) (where <em>s</em> indicates "standardized"), and our goal 
    is to show that this converges in distribution to the standard normal 
    for large enough \(n\). That is, 
    \(\bar{X}_s \xrightarrow{d} \mathcal{N}(0,1)\).
  </p>
  
  <p>
    Now rather than dealing with sample means directly, we can discuss 
    \(S\), the sum of our sample of iid random variables, 
    \(\sum_{i=1}^{n} X_i\). The sampling distribution of the sample sums 
    \(S\) has mean 
    \(E[S] = E\Bigl(\sum_{i=1}^{n} X_i\Bigr) = \sum_{i=1}^{n} E[X_i] = n\mu\) 
    and variance 
    \(\mathrm{Var}(S) = \sum_{i=1}^{n} \mathrm{Var}(X_i) = n\sigma^2\) 
    respectively. Standardizing the sample sum gives:
  </p>
  
  <p style="text-align:center;">
    \[
      S^* = \frac{S - n\mu}{\sqrt{n}\sigma}
    \]
  </p>
  
  <p style="text-align:center;">
    \[
      = \frac{\sum_{i=1}^{n} X_i - n\mu}{\sqrt{n}\sigma}
    \]
  </p>

  <p style="text-align:center;">
    \[
      = \frac{\tfrac{1}{\sqrt{n}}\,\bigl(\sum_{i=1}^{n} X_i - n\mu\bigr)}{\sigma}
    \]
  </p>
  
  <p style="text-align:center;">
    \[
      = \frac{\tfrac{1}{\sqrt{n}} \sum_{i=1}^{n} X_i - \sqrt{n}\,\mu}{\sigma}
    \]
  </p>

  <p>
    We can see this is equivalent to the standardized sample mean. Thus, 
    proving that sample sums converge to the standard normal distribution 
    also proves that the distribution of sample means does, and hence 
    proves the CLT.
  </p>
  
  <p>
    We now consider the standard normal distribution and its moment 
    generating function. The density function for the standard normal 
    distribution is given by 
    \[
      f_Z(x) = f\bigl(x \mid \mu=0,\sigma^2=1\bigr) 
      = \frac{1}{\sqrt{2\pi}} e^{-\tfrac{x^2}{2}}
    \]
    for some standard normal random variable \(Z\). Then
    \[
      M_Z(t) = E\bigl[e^{Xt}\bigr] 
      = \int_{-\infty}^{\infty} e^{xt}\,\frac{1}{\sqrt{2\pi}}\,e^{-\tfrac{x^2}{2}}\,dx.
    \]
    We can simplify this like so...
  </p>

  <p>
    First, consider the integral for the standard normal MGF:
  </p>
  
  <p style="text-align:center;">
    \[
      \int_{-\infty}^{\infty} e^{xt} \,\frac{1}{\sqrt{2\pi}}\,e^{-\frac{x^2}{2}}\,dx
    \]
  </p>

  <p style="text-align:center;">
    \[
      = \int_{-\infty}^{\infty} \frac{1}{\sqrt{2\pi}} \,
        e^{\,xt - \tfrac{x^2}{2}} \, dx
    \]
  </p>

  <p style="text-align:center;">
    \[
      = \int_{-\infty}^{\infty} \frac{1}{\sqrt{2\pi}} \,
        e^{-\tfrac{1}{2} \bigl(x^2 - 2xt\bigr)} 
    \]
  </p>

  <p style="text-align:center;">
    \[
      = \int_{-\infty}^{\infty} \frac{1}{\sqrt{2\pi}} \,
        e^{-\tfrac{1}{2}\bigl(x^2 - 2xt + t^2\bigr) 
           \;+\; \tfrac{1}{2}t^2}
    \]
  </p>

  <p style="text-align:center;">
    \[
      = \int_{-\infty}^{\infty} \frac{1}{\sqrt{2\pi}} \,
        e^{-\tfrac{1}{2}\,(x - t)^2 \;+\; \tfrac{1}{2}t^2}
    \]
  </p>

  <p>
    Since \(e^{\tfrac{1}{2}t^2}\) is a constant (no \(x\) term) we can factor it out:
  </p>

  <p style="text-align:center;">
    \[
      M_Z(t) 
      = e^{\tfrac{1}{2}t^2} \int_{-\infty}^{\infty} \frac{1}{\sqrt{2\pi}} \,
        e^{-\tfrac{1}{2}(x - t)^2} \,dx 
      = e^{\tfrac{1}{2}t^2} \cdot (1) 
      = e^{\tfrac{1}{2}t^2}.
    \]
  </p>

  <p>
    Thus \(M_Z(t) = e^{\tfrac{1}{2}t^2}\) is the moment generating function for the 
    standard normal distribution. The integral evaluates to \(1\) because itâ€™s the 
    total area under a normal density. To prove the CLT, we must show the MGF for 
    our standardized sum \(S^*\) converges to \(e^{\tfrac{1}{2}t^2}\).
  </p>

  <p>
    Recall some properties of MGFs: if \(A\) and \(B\) are independent random 
    variables with MGFs \(M_A(t)\) and \(M_B(t)\), then \(C = A + B\) has MGF 
    \(M_C(t) = M_A(t) \, M_B(t)\). Also, the \(k\)th derivative of an MGF evaluated 
    at \(t=0\) gives the \(k\)th moment of the distribution.
  </p>

  <p>
    Define the standardized sample sum 
    \[
      S^* = \frac{S - n\mu}{\sqrt{n}\,\sigma} 
      = \frac{\sum_{i=1}^n X_i - n\mu}{\sqrt{n}\,\sigma}.
    \]
  </p>

  <p style="text-align:center;">
    \[
      S^* 
      = \frac{\bigl(X_1 + X_2 + \dots + X_n\bigr) - n\mu}{\sqrt{n}\,\sigma}
    \]
  </p>

  <p style="text-align:center;">
    \[
      = \frac{(X_1 - \mu) + (X_2 - \mu) + \dots + (X_n - \mu)}{\sqrt{n}\,\sigma}
    \]
  </p>

  <p style="text-align:center;">
    \[
      = \frac{X_1 - \mu}{\sqrt{n}\,\sigma}
        + \frac{X_2 - \mu}{\sqrt{n}\,\sigma}
        + \dots 
        + \frac{X_n - \mu}{\sqrt{n}\,\sigma}.
    \]
  </p>

  <p>
    Its MGF is 
    \(\displaystyle M_{S^*}(t) = E\!\bigl[e^{\,t\,S^*}\bigr]\), and by factoring 
    out each exponent term,
  </p>

  <p style="text-align:center;">
    \[
      E\bigl[e^{t \, S^*}\bigr]
      = E\!\Bigl[e^{\,t\,\bigl(\frac{X_1-\mu}{\sqrt{n}\sigma} 
                            + \cdots + 
                            \frac{X_n-\mu}{\sqrt{n}\sigma}\bigr)}\Bigr] 
      = \bigl(E[e^{\,t\,(\frac{X_i-\mu}{\sqrt{n}\sigma})}]\bigr)^n.
    \]
  </p>

  <p>
    Each factor 
    \(E[e^{\,t\,(\tfrac{X_i-\mu}{\sqrt{n}\sigma})}]\) is an MGF of the 
    standardized variable \(\tfrac{X_i - \mu}{\sqrt{n}\sigma}\).  
    By properties of MGFs (scaling by a constant), we can write:
  </p>

  <p style="text-align:center;">
    \[
      M_{S^*}(t) 
      = \Bigl(M_{(X_i - \mu)}\bigl(\tfrac{t}{\sqrt{n}\sigma}\bigr)\Bigr)^n.
    \]
  </p>

  <p>
    Next, we expand \(M_{(X_i - \mu)}\bigl(\tfrac{t}{\sqrt{n}\sigma}\bigr)\) 
    in a Taylor series (keeping only up to the quadratic term):
  </p>

  <p style="text-align:center;">
    \[
      M_{(X_i - \mu)}\bigl(\tfrac{t}{\sqrt{n}\sigma}\bigr)
      \approx E\bigl[\,1 
        + \bigl(\tfrac{t}{\sqrt{n}\sigma}\bigr)(X - \mu)
        + \bigl(\tfrac{t^2}{2n\sigma^2}\bigr)(X - \mu)^2
        + \dots \bigr].
    \]
  </p>

  <p style="text-align:center;">
    \[
      = 1 
        + \bigl(\tfrac{t}{\sqrt{n}\sigma}\bigr)\bigl(E[X] - \mu\bigr)
        + \bigl(\tfrac{t^2}{2n\sigma^2}\bigr)\mathrm{Var}(X).
    \]
  </p>

  <p style="text-align:center;">
    \[
      = 1 
        + \frac{t}{\sqrt{n}\sigma}(\mu - \mu) 
        + \frac{t^2}{2n\sigma^2}\,\sigma^2
      = 1 + \frac{t^2}{2n}.
    \]
  </p>

  <p>
    Therefore,
  </p>

  <p style="text-align:center;">
    \[
      M_{S^*}(t) 
      = \Bigl(1 + \frac{t^2}{2n}\Bigr)^n.
    \]
  </p>

  <p>
    Recognizing the classic limit \(\lim_{n \to \infty} \bigl(1 + \frac{x}{n}\bigr)^n = e^x\), 
    we see:
  </p>

  <p style="text-align:center;">
    \[
      \lim_{n \to \infty} M_{S^*}(t) 
      = \lim_{n \to \infty} \Bigl(1 + \frac{t^2}{2n}\Bigr)^n 
      = e^{\tfrac{t^2}{2}}
      = M_Z(t).
    \]
  </p>

  <p>
    Hence, the MGF of the standardized sum converges to the MGF of the standard 
    normal distribution, completing the proof of the Central Limit Theorem.
  </p>
  
  <p>
  
  <!-- Link back to the Home page -->
  <p><a href="index.html">Return to Home Page</a></p>
</body>
</html>
